{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bart_scisum",
      "provenance": [],
      "collapsed_sections": [
        "-YqJ68pEB-_g"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oldaandozerskaya/Bulletin_of_opposition/blob/master/bart_scisum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkMFfykaWmx-"
      },
      "source": [
        "#Import libs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Goj88fbbQ3Wo"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7EVTfMVdv4Z"
      },
      "source": [
        "!pip install -q pytorch-lightning==1.2.9\n",
        "!pip install -q transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fDnjiDoeZbK"
      },
      "source": [
        "import transformers\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split, RandomSampler, Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "\n",
        "import math\n",
        "import random\n",
        "import re\n",
        "import argparse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YqJ68pEB-_g"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPxWynRmeGlL"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class LitModel(pl.LightningModule):\n",
        "  def __init__(self, learning_rate, tokenizer, model, hparams):\n",
        "    super().__init__()\n",
        "    self.tokenizer = tokenizer\n",
        "    self.model = model\n",
        "\n",
        "    self.learning_rate = learning_rate\n",
        "    self.hparams = hparams\n",
        "\n",
        "    if self.hparams.freeze_encoder:\n",
        "      freeze_params(self.model.get_encoder())\n",
        "\n",
        "    if self.hparams.freeze_embeds:\n",
        "      self.freeze_embeds()\n",
        "  \n",
        "  def freeze_embeds(self):\n",
        "    freeze_params(self.model.model.shared)\n",
        "    for d in [self.model.model.encoder, self.model.model.decoder]:\n",
        "      freeze_params(d.embed_positions)\n",
        "      freeze_params(d.embed_tokens)\n",
        "\n",
        "  def forward(self, input_ids, **kwargs):\n",
        "    return self.model(input_ids, **kwargs)\n",
        "  \n",
        "  def configure_optimizers(self):\n",
        "    optimizer = torch.optim.Adam(self.parameters(), lr = self.learning_rate)\n",
        "    return optimizer\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    src_ids, src_mask = batch[0], batch[1]\n",
        "    tgt_ids = batch[2]\n",
        "    decoder_input_ids = shift_tokens_right(tgt_ids, tokenizer.pad_token_id)\n",
        "    outputs = self(src_ids, attention_mask=src_mask, decoder_input_ids=decoder_input_ids, use_cache=False)\n",
        "    lm_logits = outputs[0]\n",
        "    ce_loss_fct = torch.nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)\n",
        "    loss = ce_loss_fct(lm_logits.view(-1, lm_logits.shape[-1]), tgt_ids.view(-1))\n",
        "    return {'loss':loss}\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "\n",
        "    src_ids, src_mask = batch[0], batch[1]\n",
        "    tgt_ids = batch[2]\n",
        "\n",
        "    decoder_input_ids = shift_tokens_right(tgt_ids, tokenizer.pad_token_id)\n",
        "    \n",
        "    outputs = self(src_ids, attention_mask=src_mask, decoder_input_ids=decoder_input_ids, use_cache=False)\n",
        "    lm_logits = outputs[0]\n",
        "\n",
        "    ce_loss_fct = torch.nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)\n",
        "    val_loss = ce_loss_fct(lm_logits.view(-1, lm_logits.shape[-1]), tgt_ids.view(-1))\n",
        "\n",
        "    return {'loss': val_loss}\n",
        "  \n",
        "  def generate_text(self, text, eval_beams, early_stopping = True, max_len = 64):\n",
        "    generated_ids = self.model.generate(\n",
        "        text[\"input_ids\"],\n",
        "        attention_mask=text[\"attention_mask\"],\n",
        "        use_cache=True,\n",
        "        decoder_start_token_id = self.tokenizer.pad_token_id,\n",
        "        num_beams= eval_beams,\n",
        "        max_length = max_len,\n",
        "        early_stopping = early_stopping\n",
        "    )\n",
        "    return [self.tokenizer.decode(w, skip_special_tokens=True, clean_up_tokenization_spaces=True) for w in generated_ids]\n",
        "\n",
        "def freeze_params(model):\n",
        "  for layer in model.parameters():\n",
        "    layer.requires_grade = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cUIEJBIjjNJ"
      },
      "source": [
        "class SummaryDataModule(pl.LightningDataModule):\n",
        "  def __init__(self, tokenizer, data_file, dev_file, batch_size, num_examples = 110000):\n",
        "    super().__init__()\n",
        "    self.tokenizer = tokenizer\n",
        "    self.data_file = data_file\n",
        "    self.dev_file = dev_file\n",
        "    self.batch_size = batch_size\n",
        "    self.num_examples = num_examples\n",
        "  \n",
        "  def prepare_data(self):\n",
        "    self.data = pd.read_csv(self.data_file, sep = '\\t', index_col = 0)#[:20000]#, header = 1)#[:self.num_examples]\n",
        "    #print(self.data.abstract.values[:5])\n",
        "    self.train = self.data.sample(frac=1, random_state = 42)\n",
        "    self.data = pd.read_csv(self.dev_file, sep = '\\t', index_col = 0)#, header = 1)#[:self.num_examples]\n",
        "    self.validate = self.data.sample(frac=1, random_state = 42)\n",
        "    #self.train, self.validate, self.test = np.split(self.data.sample(frac=1, random_state = 42), [int(.8*len(self.data)), int(.9*len(self.data))])\n",
        "    \n",
        "    self.train = encode_sentences(self.tokenizer, self.train['abstract'], self.train['title'])\n",
        "    self.validate = encode_sentences(self.tokenizer, self.validate['abstract'], self.validate['title'])\n",
        "    #self.test = encode_sentences(self.tokenizer, self.test['abstract'], self.test['title'])\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    dataset = TensorDataset(self.train['input_ids'], self.train['attention_mask'], self.train['labels'])                          \n",
        "    train_data = DataLoader(dataset, sampler = RandomSampler(dataset), batch_size = self.batch_size)\n",
        "    return train_data\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    dataset = TensorDataset(self.validate['input_ids'], self.validate['attention_mask'], self.validate['labels']) \n",
        "    val_data = DataLoader(dataset, batch_size = self.batch_size)                       \n",
        "    return val_data\n",
        "\n",
        "  '''\n",
        "  def test_dataloader(self):\n",
        "    dataset = TensorDataset(self.test['input_ids'], self.test['attention_mask'], self.test['labels']) \n",
        "    test_data = DataLoader(dataset, batch_size = self.batch_size)                   \n",
        "    return test_data\n",
        "  '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibsTjaBjZ-OE"
      },
      "source": [
        "hparams = argparse.Namespace()\n",
        "\n",
        "hparams.freeze_encoder = True\n",
        "hparams.freeze_embeds = True\n",
        "hparams.eval_beams = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "no6DwOqaE9Jw"
      },
      "source": [
        "def shift_tokens_right(input_ids, pad_token_id):\n",
        "  prev_output_tokens = input_ids.clone()\n",
        "  index_of_eos = (input_ids.ne(pad_token_id).sum(dim=1) - 1).unsqueeze(-1)\n",
        "  prev_output_tokens[:, 0] = input_ids.gather(1, index_of_eos).squeeze()\n",
        "  prev_output_tokens[:, 1:] = input_ids[:, :-1]\n",
        "  return prev_output_tokens\n",
        "\n",
        "def encode_sentences(tokenizer, source_sentences, target_sentences, max_length=512, pad_to_max_length=True, return_tensors=\"pt\"):\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "  target_ids = []\n",
        "  tokenized_sentences = {}\n",
        "\n",
        "  for sentence in source_sentences:\n",
        "    encoded_dict = tokenizer(\n",
        "          sentence,\n",
        "          max_length=max_length,\n",
        "          padding=\"max_length\" if pad_to_max_length else None,\n",
        "          truncation=True,\n",
        "          return_tensors=return_tensors,\n",
        "          add_prefix_space = True\n",
        "      )\n",
        "\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "  input_ids = torch.cat(input_ids, dim = 0)\n",
        "  attention_masks = torch.cat(attention_masks, dim = 0)\n",
        "\n",
        "  for sentence in target_sentences:\n",
        "    encoded_dict = tokenizer(\n",
        "          sentence,\n",
        "          max_length=max_length,\n",
        "          padding=\"max_length\" if pad_to_max_length else None,\n",
        "          truncation=True,\n",
        "          return_tensors=return_tensors,\n",
        "          add_prefix_space = True\n",
        "      )\n",
        "    # Shift the target ids to the right\n",
        "    # shifted_target_ids = shift_tokens_right(encoded_dict['input_ids'], tokenizer.pad_token_id)\n",
        "    target_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "  target_ids = torch.cat(target_ids, dim = 0)\n",
        "  \n",
        "  batch = {\n",
        "      \"input_ids\": input_ids,\n",
        "      \"attention_mask\": attention_masks,\n",
        "      \"labels\": target_ids,\n",
        "  }\n",
        "\n",
        "  return batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "510B_rmRFAE8"
      },
      "source": [
        "# Load BART\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUiE1G90Jedf"
      },
      "source": [
        "# Load the model\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, AdamW, BartConfig\n",
        "\n",
        "#tokenizer = BartTokenizer.from_pretrained('sshleifer/distilbart-cnn-12-6', add_prefix_space=True)\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base', add_prefix_space=True)\n",
        "#tokenizer = BartTokenizer.from_pretrained(\"VictorSanh/bart-base-finetuned-xsum\", add_prefix_space=True)\n",
        "\n",
        "bart_model = BartForConditionalGeneration.from_pretrained(\n",
        "    #\"VictorSanh/bart-base-finetuned-xsum\")\n",
        "    \"facebook/bart-base\")\n",
        "    #\"sshleifer/distilbart-cnn-12-6\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_h8QhLcyh9RJ"
      },
      "source": [
        "# Load the data into the model for training\n",
        "summary_data = SummaryDataModule(tokenizer, '/content/gdrive/MyDrive/DAAD/Datasets_DAAD/SciTLDR/arXiv_train_q1.csv', \\\n",
        "                                  '/content/gdrive/MyDrive/DAAD/Datasets_DAAD/arXiv/arXiv_dev.csv', batch_size = 4, num_examples = 20000)\n",
        "\n",
        "# Load the model from checkpoint\n",
        "#model = LitModel.load_from_checkpoint(path,\n",
        "                                       #learning_rate = 2e-5, tokenizer = tokenizer, model = bart_model, hparams = hparams)\n",
        "\n",
        "model = LitModel(learning_rate = 2e-5, tokenizer = tokenizer, model = bart_model, hparams = hparams)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FM6puw22w-5n"
      },
      "source": [
        "#trainer.save_checkpoint(\"/content/gdrive/MyDrive/BART_results/1/model.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xcEqNMdGa6i"
      },
      "source": [
        "# Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAj9wgyRXbRG"
      },
      "source": [
        "checkpoint = ModelCheckpoint('/content/gdrive/MyDrive/Untitled Folder/checkpoint_files_2')\n",
        "trainer = pl.Trainer(gpus = 1,\n",
        "                     max_epochs = 2,\n",
        "                     min_epochs = 1,\n",
        "                     auto_lr_find = False,\n",
        "                     checkpoint_callback = checkpoint,\n",
        "                     progress_bar_refresh_rate = 500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWccEjPWwhHW"
      },
      "source": [
        "trainer.fit(model, summary_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t77cjYY_fZlb"
      },
      "source": [
        "del(summary_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTgrdG4eHMKV"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC7zRe5HtY7K"
      },
      "source": [
        "model.to(torch.device('cpu'))\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jkcbAZsU-0t"
      },
      "source": [
        "!pip install rouge-score\n",
        "from rouge_score import rouge_scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "\n",
        "scores = scorer.score('The quick brown fox jumps over the lazy dog',\n",
        "                      'The quick brown dog jumps on the log.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUWz3Gklpye7"
      },
      "source": [
        "generated_tldr = []\n",
        "\n",
        "path = ''\n",
        "path_save = ''\n",
        "\n",
        "test = pd.read_csv(path, sep = '\\t', index_col = 0)\n",
        "originals = test.title.values\n",
        "\n",
        "for i, line in enumerate(test.values):#test.values\n",
        "  if i%200 == 0:\n",
        "    print(i)\n",
        "    #print(originals[i])\n",
        "    print(line)\n",
        "  prompt_line_tokens = tokenizer(line[1], max_length = 512, return_tensors = \"pt\", truncation = True)\n",
        "  line_ = model.generate_text(prompt_line_tokens, eval_beams = 4, max_len = 512)\n",
        "  generated_tldr.append(line_)\n",
        "  \n",
        "\n",
        "with open(path_save, 'wb') as fp:\n",
        "    pickle.dump(generated_tldr, fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwWxr74VCnCc"
      },
      "source": [
        "rouge1 = 0\n",
        "rouge2 = 0\n",
        "rougel = 0\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "for i in range(len(generated)):\n",
        "  \n",
        "  scores = scorer.score(originals_[i].replace('\\n','').lower(), generated[i][0].replace('\\n','').lower())\n",
        "  #scores = scorer.score(originals[i].replace('\\n','').lower(), first_sentences[i].replace('\\n','').lower())\n",
        "  rouge1+=scores['rouge1'][2]\n",
        "  rouge2+=scores['rouge2'][2]\n",
        "  rougel+=scores['rougeL'][2]\n",
        "\n",
        "print(rouge1/len(generated))\n",
        "print(rouge2/len(generated))\n",
        "print(rougel/len(generated))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}